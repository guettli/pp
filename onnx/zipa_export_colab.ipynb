{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZIPA ONNX Export\n",
    "\n",
    "This notebook exports the ZIPA small CTC model to ONNX format.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (free tier works)\n",
    "- ~10 minutes to complete\n",
    "\n",
    "**Output:**\n",
    "- `model.onnx` (~250MB) - FP32 ONNX model\n",
    "- `vocab.json` - 127 IPA tokens\n",
    "\n",
    "**Instructions:**\n",
    "1. Runtime â†’ Change runtime type â†’ **T4 GPU**\n",
    "2. Runtime â†’ Run all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Sync from GitHub\n",
    "if [ ! -d \"pp\" ]; then\n",
    "    git clone https://github.com/guettli/pp.git\n",
    "    echo \"Cloned pp repo\"\n",
    "else\n",
    "    cd pp && git pull\n",
    "    echo \"Updated pp repo\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability (without importing torch yet)\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'],\n",
    "                       capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"No GPU found! Go to Runtime â†’ Change runtime type â†’ T4 GPU\")\n",
    "print(f\"GPU: {result.stdout.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Check if already installed correctly\n",
    "python -c \"import torch; assert torch.__version__.startswith('2.4.0')\" 2>/dev/null && {\n",
    "    echo \"PyTorch 2.4.0 already installed, skipping...\"\n",
    "    exit 0\n",
    "}\n",
    "\n",
    "# Bash Strict Mode\n",
    "trap 'echo -e \"\\nðŸ¤· ðŸš¨ ðŸ”¥ Warning: A command has failed. Exiting the script. Line was ($0:$LINENO): $(sed -n \"${LINENO}p\" \"$0\" 2>/dev/null || true) ðŸ”¥ ðŸš¨ ðŸ¤· \"; exit 3' ERR\n",
    "set -Eeuo pipefail\n",
    "\n",
    "echo \"=== Uninstalling incompatible packages ===\"\n",
    "pip uninstall -y k2 torch torchaudio torchvision 2>/dev/null || true\n",
    "\n",
    "echo \"=== Installing PyTorch 2.4.0 + CUDA 12.4 ===\"\n",
    "pip install torch==2.4.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "echo \"=== Installing k2 ===\"\n",
    "pip install k2==1.24.4.dev20241030+cuda12.4.torch2.4.0 -f https://k2-fsa.github.io/k2/cuda.html\n",
    "\n",
    "echo \"=== Installing icefall and other dependencies ===\"\n",
    "pip install git+https://github.com/k2-fsa/icefall.git\n",
    "pip install lhotse --no-deps\n",
    "pip install cytoolz intervaltree lilcom audioread soundfile\n",
    "pip install sentencepiece onnx onnxruntime\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Dependencies installed! ===\"\n",
    "echo \"=== Runtime will restart in next cell. Then re-run all cells. ===\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart runtime to load new PyTorch version (skips if already correct)\n",
    "import torch\n",
    "if not torch.__version__.startswith('2.4.0'):\n",
    "    print(\"Restarting runtime to load PyTorch 2.4.0...\")\n",
    "    print(\"After restart, click 'Runtime â†’ Run all' again.\")\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)\n",
    "else:\n",
    "    print(f\"PyTorch {torch.__version__} already loaded, no restart needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "import torch\n",
    "import k2\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "#print(f\"k2: {k2.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone ZIPA Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Bash Strict Mode\n",
    "trap 'echo -e \"\\nðŸ¤· ðŸš¨ ðŸ”¥ Warning: A command has failed. Exiting the script. Line was ($0:$LINENO): $(sed -n \"${LINENO}p\" \"$0\" 2>/dev/null || true) ðŸ”¥ ðŸš¨ ðŸ¤· \"; exit 3' ERR\n",
    "set -Eeuo pipefail\n",
    "\n",
    "if [ ! -d \"zipa\" ]; then\n",
    "    git clone --depth 1 https://github.com/lingjzhu/zipa.git\n",
    "    echo \"ZIPA repo cloned!\"\n",
    "else\n",
    "    echo \"ZIPA repo already exists\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"zipa-export/exp\", exist_ok=True)\n",
    "\n",
    "# Download model checkpoint\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"anyspeech/zipa-small-crctc-500k\",\n",
    "    filename=\"zipa_small_crctc_500000_avg10.pth\",\n",
    "    local_dir=\"zipa-export\"\n",
    ")\n",
    "print(f\"Model downloaded: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap checkpoint in icefall format and download tokenizer\n",
    "import torch\n",
    "\n",
    "# Load the direct state dict and wrap it for icefall\n",
    "checkpoint = torch.load(\"zipa-export/zipa_small_crctc_500000_avg10.pth\", map_location=\"cpu\")\n",
    "wrapped = {\"model\": checkpoint}\n",
    "torch.save(wrapped, \"zipa-export/exp/epoch-999.pt\")\n",
    "print(\"Checkpoint wrapped and saved!\")\n",
    "\n",
    "# Download tokenizer\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/lingjzhu/zipa/main/ipa_simplified/bpe.model\",\n",
    "    \"zipa-export/bpe.model\"\n",
    ")\n",
    "print(\"Tokenizer downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokens.txt from BPE model\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"zipa-export/bpe.model\")\n",
    "\n",
    "with open(\"zipa-export/tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(sp.GetPieceSize()):\n",
    "        f.write(f\"{sp.IdToPiece(i)} {i}\\n\")\n",
    "\n",
    "print(f\"Created tokens.txt with {sp.GetPieceSize()} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Bash Strict Mode\n",
    "trap 'echo -e \"\\nðŸ¤· ðŸš¨ ðŸ”¥ Warning: A command has failed. Exiting the script. Line was ($0:$LINENO): $(sed -n \"${LINENO}p\" \"$0\" 2>/dev/null || true) ðŸ”¥ ðŸš¨ ðŸ¤· \"; exit 3' ERR\n",
    "set -Eeuo pipefail\n",
    "\n",
    "cd zipa/zipformer_crctc\n",
    "\n",
    "# ZIPA small model parameters (from zipa_ctc_inference.py)\n",
    "python export-onnx-ctc.py \\\n",
    "    --exp-dir ../../zipa-export/exp \\\n",
    "    --tokens ../../zipa-export/tokens.txt \\\n",
    "    --epoch 999 \\\n",
    "    --avg 1 \\\n",
    "    --use-averaged-model 0 \\\n",
    "    --num-encoder-layers \"2,2,3,4,3,2\" \\\n",
    "    --feedforward-dim \"512,768,1024,1536,1024,768\" \\\n",
    "    --encoder-dim \"192,256,384,512,384,256\" \\\n",
    "    --encoder-unmasked-dim \"192,192,256,256,256,192\" \\\n",
    "    --num-heads \"4,4,4,8,4,4\" \\\n",
    "    --cnn-module-kernel \"31,31,15,15,15,31\" \\\n",
    "    --query-head-dim 32 \\\n",
    "    --value-head-dim 12 \\\n",
    "    --pos-head-dim 4 \\\n",
    "    --pos-dim 48 \\\n",
    "    --downsampling-factor \"1,2,4,8,4,2\" \\\n",
    "    --causal False \\\n",
    "    --use-transducer 0 \\\n",
    "    --use-ctc 1\n",
    "\n",
    "echo \"ONNX export complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create vocab.json for Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert tokens.txt to vocab.json\n",
    "vocab = {}\n",
    "with open(\"zipa-export/tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\" \")\n",
    "        if len(parts) == 2:\n",
    "            token, idx = parts\n",
    "            vocab[token] = int(idx)\n",
    "\n",
    "with open(\"zipa-export/exp/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Created vocab.json with {len(vocab)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validate and Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List output files\n",
    "print(\"Output files:\")\n",
    "for f in os.listdir(\"zipa-export/exp\"):\n",
    "    path = f\"zipa-export/exp/{f}\"\n",
    "    if os.path.isfile(path):\n",
    "        size_mb = os.path.getsize(path) / 1024 / 1024\n",
    "        print(f\"  {f}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate ONNX model\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "session = ort.InferenceSession(\"zipa-export/exp/model.onnx\")\n",
    "\n",
    "print(\"Model inputs:\")\n",
    "for inp in session.get_inputs():\n",
    "    print(f\"  {inp.name}: {inp.shape}\")\n",
    "\n",
    "print(\"\\nModel outputs:\")\n",
    "for out in session.get_outputs():\n",
    "    print(f\"  {out.name}: {out.shape}\")\n",
    "\n",
    "# Test inference\n",
    "x = np.random.randn(1, 100, 80).astype(np.float32)\n",
    "x_lens = np.array([100], dtype=np.int64)\n",
    "outputs = session.run(None, {\"x\": x, \"x_lens\": x_lens})\n",
    "print(f\"\\nTest inference successful! Output shape: {outputs[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Bash Strict Mode\n",
    "trap 'echo -e \"\\nðŸ¤· ðŸš¨ ðŸ”¥ Warning: A command has failed. Exiting the script. Line was ($0:$LINENO): $(sed -n \"${LINENO}p\" \"$0\" 2>/dev/null || true) ðŸ”¥ ðŸš¨ ðŸ¤· \"; exit 3' ERR\n",
    "set -Eeuo pipefail\n",
    "\n",
    "cd zipa-export/exp\n",
    "zip -r ../../zipa-onnx-export.zip model.onnx vocab.json\n",
    "\n",
    "echo \"\"\n",
    "echo \"==================================================\"\n",
    "echo \"EXPORT COMPLETE!\"\n",
    "echo \"==================================================\"\n",
    "echo \"\"\n",
    "echo \"Download zipa-onnx-export.zip from the file browser\"\n",
    "echo \"(left panel) or run the next cell.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download(\"zipa-onnx-export.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
